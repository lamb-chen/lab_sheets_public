





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from skimage import io
from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error





# write your code here

##CORRECT ANSWER##

A = np.array([[2, 3], [4, -1], [5, 6]])
B = np.array([[5, 2], [8, 9], [2, 1]])

C = 3 * A
print('C1:')
print(C)

C = A + B
print('C2:')
print(C)

C = np.dot(A, B.T)
print('C3:')
print(C)

C = np.multiply(A, B)
print('C4:')
print(C)

##CORRECT ANSWER##





# write your code here

##CORRECT ANSWER##

Asum = A.sum()
print('A sum:', Asum)

Amean = A.mean()
print('A mean:', Amean)

Avar = A.var()
print('A var: ', Avar)

Asum_row = A.sum(axis=1)
print('A sum (row):', Asum_row)

Asum_col = A.sum(axis=0)
print('A sum (column):', Asum_col)
      
##CORRECT ANSWER##





# write your code here
    
##CORRECT ANSWER##

def sigmoid(x):
    return 1/(1+np.exp(-x))
    
##CORRECT ANSWER##





# write your code here
    
##CORRECT ANSWER##

A = np.array([-5, 0, 5])
print(sigmoid(A))

x = np.linspace(-5, 5, 100)
fig, ax = plt.subplots()
ax.plot(x, sigmoid(x))
ax.set_xlabel('x')
ax.set_ylabel('sigmoid(x)')

##CORRECT ANSWER##





# write your code here
    
##CORRECT ANSWER##

def standardiseCols(x):
    x_means = x.mean(axis=0, keepdims=True)
    x_std = x.std(axis=0, keepdims=True)
    return (x - x_means) / x_std

### END CODE HERE ###





# write your code here
    
##CORRECT ANSWER##

x = np.array([
    [0, 3, 5],
    [1, 6, 4],
    [3, -2, 8],
    [-1, 1, 10]
])
x_standard = standardiseCols(x)
print(x_standard.mean(axis=0))
print(x_standard.std(axis=0))

##CORRECT ANSWER##





image = io.imread('flower.png')
io.imshow(image)
image.shape





# write your code here
    
##CORRECT ANSWER##

v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2], 1)
# or v = image.reshape(-1,1) where the -1 dimension is inferred  
v.shape

##CORRECT ANSWER##





a = np.arange(5)
print(a)
print(a.shape)
print(a.T)
print(a.T.shape)





a = a.reshape(1,-1)
print(a)
a.shape





a = a.reshape(-1,1)
print(a)
a.shape





assert(a.shape == (5,1))








# write your code here
    
##CORRECT ANSWER##

X, y = fetch_california_housing(return_X_y=True)
print(X.shape[0], 'examples')
print(X.shape[1], 'features')
feature_names = fetch_california_housing().feature_names
print(feature_names)

data = pd.DataFrame(X, columns=feature_names)
data['MedHouseValue'] = y
plot = sns.pairplot(data)

##CORRECT ANSWER##








# write your code here
    
##CORRECT ANSWER##

scaler = StandardScaler().fit(X)
X_scaled = scaler.transform(X)
print(X_scaled.mean(axis=0))
print(X_scaled.std(axis=0))

##CORRECT ANSWER##





# write your code here

##CORRECT ANSWER##

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
print('Training set size:', len(X_train))
print('Test set size:', len(X_test))

##CORRECT ANSWER##





# write your code here

##CORRECT ANSWER##

lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
print(lin_reg.coef_)

##CORRECT ANSWER##





# write your code here

##CORRECT ANSWER##

RMSE_train = np.sqrt(mean_squared_error(y_train, lin_reg.predict(X_train)))
print('Train RMSE: ', round(RMSE_train,2))
RMSE_test = np.sqrt(mean_squared_error(y_test, lin_reg.predict(X_test)))
print('Test RMSE: ', round(RMSE_test,2))

fig, ax = plt.subplots(figsize=(6,6))
ax.scatter(y_test, lin_reg.predict(X_test))
ax.plot([0,5],[0,5])
ax.set_xlabel('Real value')
ax.set_ylabel('Prediction')

##CORRECT ANSWER##





np.random.seed(0)
N = 30
sigma = 5
x = np.sort(np.random.sample((N,1)))*10
y = (x-1)*(x-5) + np.random.normal(0,sigma,N).reshape(-1, 1)

fig, ax = plt.subplots(figsize=(6,4))
ax.scatter(x, y)
ax.set_xlabel('x')
ax.set_ylabel('y')





# write your code here

##CORRECT ANSWER##

x_bias = np.concatenate((np.ones(N).reshape(-1,1), x), axis=1)

w_fit = np.linalg.lstsq(x_bias, y, rcond=None)[0]
y_pred = np.dot(x_bias, w_fit)

print('w_1 = {:.2f}'.format(w_fit[1].item()))
print('b = {:.2f}'.format(w_fit[0].item()))

fig, ax = plt.subplots(figsize=(6,4))
ax.scatter(x, y)
ax.plot(x, y_pred, '-r')
ax.set_xlabel('x')
ax.set_ylabel('y')

##CORRECT ANSWER##





# write your code here

##CORRECT ANSWER##

regr = LinearRegression()
regr.fit(x, y)
print('w_1 = {:.2f}'.format(regr.coef_.item()))
print('b = {:.2f}'.format(regr.intercept_.item()))

##CORRECT ANSWER##





# write your code here

##CORRECT ANSWER##

poly = PolynomialFeatures(degree=2)
x_poly = poly.fit_transform(x)
regr.fit(x_poly, y)

x_plot = np.linspace(0,10,200).reshape(-1, 1)
y_pred = regr.predict(poly.transform(x_plot))

fig, ax = plt.subplots(figsize=(6,4))
ax.scatter(x, y);
ax.plot(x_plot, y_pred, '-r')
ax.set_xlabel('x')
ax.set_ylabel('y')

##CORRECT ANSWER##






